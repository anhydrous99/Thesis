\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}

\usepackage{mathtools}
\usepackage{tabularx}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Artificial Intelligence to Reinforcement Learning}
\author{Armando Herrera}

\begin{document}
\maketitle

Here I'll go over the fundamentals of Neural Network and Machine Learning as well as the some applications and practical applications. Then I'll go over the fundamentals of Reinforcement Learning from the typical formalism to some of the latest methods and algorithms.

Before we even go over neural network, I do believe in having a strong foundation in the subject and a clear mathematical formalism. This allows for a subject's mathematical description of methods to speak to us in a clear and understandable language forming a clear picture.

\section{Introductory Mathematics}

\subsection{Formalism}

In this paper, I heavily rely on the formalism typically expressed in linear algebra textbooks. This allows me to relay the most important information, intuitively, while preserving exactness.

To describe a scalar, mathematically, a standard letter a used. For instance $a=1$, where I assign the variable $a$ the scalar value of $1$. To describe a vector, an arrow is placed above a variable to denote it as a vector, $\vec{v}=\vec{0}$ denotes a variable $\vec{v}$ assigned the zero vector. A vector can be described as an element of n-dimensional Euclidean Space where each element of the vector denotes an axis of said space. $$\vec{x}=\begin{bmatrix}1 & 2 & 3\end{bmatrix}$$ is an example vector in 3 dimensional euclidean space. This same vector is denoted as a row-wise vector, where all the elements of the vector are on a single row, a column-wise vector would look like $$\vec{x}=\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}.$$ The significance of the distinctions between a row-wise and column-wise vector are show when going over the matrix as in some operations they have different results and making a distinction matters. From now on when a vector is shown and is not other wise said assume the vector is in column-wise form. 

Another form that a vector can be expressed in is via summation notation. In this notation, we must first define the following unit vectors, $$\hat{r}_1=\begin{bmatrix}1 \\ 0 \\ \vdots \\ 0\end{bmatrix},\hat{r}_2=\begin{bmatrix}0 \\ 1 \\ \vdots \\ 0\end{bmatrix},\dots,\hat{r}_n=\begin{bmatrix}0 \\ 0 \\ \vdots \\ 1\end{bmatrix} .$$ Each element of said vector can then be denoted as $$\vec{x}=x_1 \hat{r}_1 + x_2 \hat{r}_2 + \dots + x_n \hat{r}_n$$ and finally using sigma notation, $$\vec{x}=\sum_{i=1}^n x_i \hat{r}_i.$$ This notation is important to properly display information and proofs that would be difficult to display using the previously show forms. One example proof is the proof for the vector's associativity axiom, $\vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}$.
\begin{proof}
\begin{align*}
\vec{u}+(\vec{v}+\vec{w})&=(\vec{u}+\vec{v})+\vec{w} \\
&=\bigg(\sum_{i=1}^nu_i \hat{r}_i+\sum_{j=1}^n v_j \hat{r}_j\bigg)+\sum_{k=1}^n w_k \hat{r}_k\\
&=\sum_{i=1}^n (u_i\hat{r}_i + v_i\hat{r}_i) + w_i\hat{r}_i\\
&=\sum_{i=1}^n \big((u_i + v_i) + w_i\big)r_i\\
&=\sum_{i=1}^n \big(u_i+(v_i+w_i)\big)r_i&\text{scalar's associativity}\\
&=\vec{u}+(\vec{v}+\vec{w})
\end{align*}
\end{proof}

\subsection{Matrix Operations}

In this section, I will go over the common matrix operations that are used in Machine Learning. Firstly, a matrix is basically a table of mathematical expression or numbers arranged in columns and rows. For example, $$A=\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}.$$ Both row vectors and column vectors can be seen as matrices with a single row or a single column. In this paper, when ever a letter is capitalized it means that it represent a matrix. When a matrix is show as $[A]_{ij}$ this means we are talking about the element in the $i$th row and the $j$th column.

\subsubsection{Component-wise Operations}

Component wise operations like multiplication, addition, and subtraction can be denoted on a per component basis. $$[A\square B]_{ij}=[A]_{ij} \square [B]_{ij}$$

The $\square$ symbols represent the different possible component-wise operations like multiplication, add, subtraction, and even division.

\subsubsection{Matrix Multiplication}

Matrix Multiplication is the most heavily used operation in Machine Learning. It's definition is simple, in summation notation, $$[AB]_{ij}=\sum_{k=1}^n [A]_{ik}[B]_{kj}.$$

\subsubsection{Matrix Transpose}

The transpose operation is also used extensively. It is defined as $$[A^T]_{ij}=[A]_{ji}.$$

\subsection{Dot Product}

Both single rowed, and single columned matrices are vectors, which means we can borrow some of the operations, for instance, the component-wise operations.

The Dot Product is defined as $$\vec{a}\cdot \vec{b}=\sum_{i=1}^n a_i b_i.$$ Notice, the Dot Product basically is Matrix Multiplication between two vectors, one in the form of a row vector and the other as a column vector. 

\begin{proof}
\begin{align*}
\vec{a}\cdot \vec{b}&=\vec{a}\vec{b}^T\\
&=\begin{bmatrix}a_1 & a_2 & \dots \end{bmatrix}
\begin{bmatrix}b_1 \\ b_2 \\ \vdots \end{bmatrix}\\
&=\sum_{i=1}^na_i b_i \\
&=\vec{a}\cdot \vec{b}
\end{align*}
\end{proof}

\section{The Agent and it's Environment}

According to "Artificial Intelligence: A Modern Approach" by Russel and Norvig an agent as "anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators." For instance, an autonomous car will have the roads, and the area around the roads as the environment perceives through the various IR, radar, and visual sensors around the car. The autonomous car will interact with the environment through its various actuators, the accelerator, the brake, and the steering. 

In terms of the environment, I will describe the agent as a satellite that is orbiting a pseudo-earth with a heavier atmosphere, to simulate an accelerated orbital decay. The orbital decay is accelerated because the orbital decay forces are relatively minute, so creating an agent in a reasonable amount of time with a reasonable amount of computational resources is done. Of course, this is a hyper-parameter that can be changed in the future to continue research into creating a more long-term agent.  

The satellite's actuators are its engines that generate thrust and its angle of thrust. The goal of the agent would then be to try its best to maintain its orbit. The performance measure would be a function of the amount of time it stays within a certain threshold from its orbit altitude. A more detailed and mathematical description is explained in a later section.

\begin{table}
\caption{A simple description of the environment.}
\begin{tabularx}{\columnwidth}{|X||X|X|X|X|} \hline
Agent Type & Performance Measure & Environment & Actuators & Sensors \\ \hline
Satellite  & How long the satellite is able to maintain it's orbit with the amount of fuel it has. & 
Low Earth Orbit & The engine and it's angle of thrust. & Relative position, velocity data with respect to earth and it's target orbit, as well as it's current angle and thrust. \\ \hline
\end{tabularx}
\end{table}

The environment is static, single-agent, fully observable sequential, and continuous. Static in that the agent does not have to keep cognizant while it is deliberating. Single-agent, while the environment is expanded to accommodate multiple agents to accelerate training to keep it simple, it is a single-agent environment. It is sequential where the agent's experience cannot be divided into atomic episodes, and every time step requires the last. Finally, it is continuous as the position, velocity, and other attributes are continuous. The actions of thrust and angle will be continuous. 

\section{Neural Networks}

The history of neural networks goes back to the 1940 but was really popularized with the perceptron in the 50s and 60s where the perceptron was implemented on an IBM 704.

\subsection{Perceptrons}

In the simplest of thinking a perceptron simply is a single-layer neural network. It basically is a piecewise function where a weights $w$ and bias $b$ determine the output. $$f(\vec{x})=\begin{cases}
1 \mbox{ if } \vec{w}\cdot \vec{x}+b>0 \\
0 \mbox{ otherwise }
\end{cases}$$ Where $\vec{w}$  is the weight vector, $b$ is the bias, and $x$ is the input vector.  Perceptrons are a powerful concept since n-Dimensional clusters can be linearly separated. A great example where perceptrons are extremely helpful is with logical operators. A single perceptron can mimic most logical operators except for XOR; Multiple perceptrons can mimick XOR. Since the perceptron is a linear classifier, the perceptron will fail if a cluster is not linearly separable. 

One example of a logical operator that a perceptron can mimic is the AND function. The AND function is only true when both both inputs are true. Let true be one and false be zero, then if $\vec{w}=\begin{bmatrix}1 \\ 1\end{bmatrix}$ and $b=-1.5$ then the perceptron will only output one when both inputs are one.

\subsection{Feed-forward Neural Networks}

We can classify Feed-forward Neural Networks into several categories. One of these categories, the simplest, is the Single Neuron Single Input Network. 
$$a(x)=f(wx + b)|_{w.b}$$
$w$ and $b$ are weight constants, and $f$ is a transfer function that can be linear or non-linear function and whose purpose is to turn input information into output infoâ€”for example, turning input data into 0s and 1s using the piecewise function, thereby turning into a perceptron. A Single Neuron Multiple Input Neural Network is the next simplest. Here, the Neural Network can be described similarly as the perceptron except for the additional function variable, $$a(\vec{x})=f(\vec{w}\cdot \vec{x}+b).$$ The next most straightforward form of the Neural Network is the Neural Network layer, where a Neural Network has multiple inputs and multiple neurons, resulting in multiple output values, or an output vector. 
$$\vec{a}(\vec{x})=\vec{f}(W\vec{x}+\vec{b})$$ Where $W\vec{x}$ is the Matrix-Vector Multiplication, $$[W\vec{x}]_i=\sum_{j=1}^n [W]_{ij}[\vec{x}]_{j}.$$ 

A Feed-Forward Neural Network's basic anatomy is then an initial input layer, an output layer, and any number of hidden layers. Using the equations so far, it is then possible to build these FNNs. Let us consider this FNN with an arbitrary number of layers, $m$, and an arbitrary number of Neurons per layer. The layer is then 
\begin{align*}
&\text{First Layer} \\
\vec{a}_1 &= f_1(W_1\vec{x} +\vec{b}_1 )\\
&\vdots\\
\vec{a}_i &= f_i(W_i\vec{a}_{i-1}+\vec{b}_i)\\
&\vdots\\
\vec{a}_m &= f_m(W_m\vec{a}_{m-1}+\vec{b}_m)\\
\end{align*}

\subsection{Loss and Activation Functions}

\section{Back-Propagation, SGD, and ADAM}

Back-Propagation is a specific case of Automatic Differentiation (AD). AD is a Differentiation technique that takes advantage of the fact that computers evaluate expression sequentially. This fact allows for the repeated application of the chain rule. While being similar to both Symbolic Differentiation and Numerical Differentiation, it is neither. Both classical methods of differentiation are comparatively slow at calculating the gradient to complicated functions. 

The chain rule, in calculus, is a formula to calculate the derivative of a composite function. If two functions, $f$ and $g$ are composite, that composite function is $h$, and $h$'s dependent variable is $x$ then the chain rule defined as 
$$\frac{dh(x)}{dx}=\frac{d}{dx}(f(g(x)))=\frac{df}{dg}\bigg|_{g(x)}\cdot\frac{dg}{dx}\bigg|_{x}.$$ Forwards and Backwards accumulation of the computation graph is allowed using the chain rule. Many great tools exist to calculate this create and use these computation graphs via a GradientTape to calculate the gradient of arbitrary functions like PyTorch or Tensorflow python libraries.

\subsection{Stochastic Gradient Descent}

The Stochastic Gradient Descent (SGD) is a stochastic approximation of the Gradient Descent algorithm that finds the minimum or optimizes a function. In the SGD algorithm, calculate the gradient resulting from a single randomly picked sample, then multiply it by a learning rate, hyper-parameter between 0 and 1, then add it to the weights to biases. $$\vec{x}=\vec{x}-\gamma\nabla \vec{f}(\vec{x})$$ Applying Back-propagation and SGD to Neural Networks if the mean square error is used as the loss-function. results in 
\begin{align*}
W_{m}(k+1)&=W_{m}(k)-\alpha \vec{s}_m (\vec{a}_{m-1})^T\\
\vec{b}_m(k+1)&=\vec{b}_{m}(k)-\alpha \vec{s}_m
\end{align*} where
\begin{align*}
s_{M}&=-2\dot{F}_M(\vec{n}_M)(\vec{t}-\vec{a})\\
s_m&=\dot{F}_m(\vec{n}_m)(W_{m+1})^T s^{m+1}\\
\dot{F}(\vec{n}_m)&=I\dot{f}(\vec{n}_m)^T
\end{align*} and $I$ is the square  identity matrix.

\section{Reinforcement Learning}

\subsection{Cross-Entropy Method}

\subsection{Q-Learning}

\subsection{TRPO}

\subsection{DQN}

\subsection{PPO}

\end{document}